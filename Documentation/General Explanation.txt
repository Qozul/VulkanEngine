See the program flow diagram for a general overview of how the system runs. Read below for me details.

The system is designed to be able to accomodate any scene by adding a dependency between scene metadata and a large part
of the graphics engine. It is also designed to have a low total number of descriptor bindings per frame and use an indirect
rendering model to allow for efficient, scene focused frames.

At the start of initialisation, the Vulkan core is loaded and created, this includes the glfw window, Vulkan instance and surface,
Validation layers if enabled, the physical device (GPU) to use, a logical Vulkan device, and the swap chain. As well as the core requirements
for window presentation and swap chains, the Vulkan core requires that the hardware supports VK_EXT_descriptor_indexing.

Next, the scene is loaded from a .scene file in the Data/Scenes folder. There must always be a default.scene to allow the engine to run from.
The scene file describes all entities in a scene, along with any resources they need (textures, default values, specialised entities),
what components they have (graphics, game script, collision volume, rigid body), their parent entity, and tags such as OPAQUE, TRANSPARENT, 
LIGHT_SOURCE, STATIC. The loader generates a scene graph from this data, creates all of the entities with appropriate data and components, and 
determines the maximum requirements of the scene, for example if there are 10 entities with graphics components in the scene then there
needs to be enough buffer size for each entity, and if these have 7 textures between them then there needs to be enough descriptors for 7
textures. All mesh and texture data is put on a queue to be loaded.

Now, knowing what the data requirements of the scene are, the descriptors that are used by the rendering pipeline can be created, along with
their underlying data buffers. The primary descriptor pool has 2 descriptor sets covering the following descriptors:
	- Materials (set 0, binding 0)
	- Textures (which will be accessed with descriptor indexing) (set 0, binding 1)
	- Per instance data (such as model matrices and material index) (set 1, binding 0)
	- Per mesh data (anything constant across all instances of a mesh) (set 1, binding 1)
	- MVP buffer (set 1, binding 2)
The descriptor type for the buffers can be uniform or storage. Uniform buffers are typically faster due to being shader local, and will be at worst 
the same performance as storage buffers, however they are limited to a hardware-defined maximum size. Therefore, a comparison between the hardware 
limit and the maximum size defined by the scene can be made, and if all of the data can fit then uniform buffers are used, otherwise storage buffers 
will be used. The underlying buffers for these descriptors are created and can be filled. This stage of initialisation can then output the 
descriptor set layouts for later use by the renderers. Dynamic buffers are used to offset appropriately for each frame image index.

Not shown on the diagram, are the vertex and index buffer creation. These are created along with the descriptors and they can begin to be filled by 
mesh data using the queue generated by the scene. This is performed on a separate thread. There is one vertex + index buffer for all static objects
and another for all dynamic objects. The sizes of these buffers may be unknown until mesh loading is complete, at which point the buffer is
"committed" to GPU friendly memory and the size is locked in. Furthermore, textures are also loaded from disc using the queue on a separate thread.

Finally, the render passes and renderers can be created, since the layouts of the descriptors are now known. Furthermore, the maximum size of each
renderer's subbuffer in the descriptor buffers is known, and thus the offsets in to those buffers that all objects in the particular renderer need.
These are added as specialisation constants.

In future, the scene management may be updated to split up the world in to sections that reduce the maximum required sizes for descriptor buffers,
and could lead to more prevalent use of uniform buffers. Another future thing is to have LOD available and dealt with by the scene manager.
Pipeline caching for faster creation when changing scene or changing the window size.

Once initialisation is completed, the system enters it's frame loop, which consists of a traversal of the scene graph, updating game scripts, 
calculating combined transformation matrices, view frustum culling, and inserting/removing entities and their data from the RenderCommands. Then,
each renderer uses generated indirect commands from the scene traversal to quickly fill the command buffer for the frame and enqueue it.

-----

A quick look at the descriptor and element buffer system:
	- Buffers are all global and 2 or 3 times the size of their required data, to allow updates on each frame.
	- Because buffers are global they must hold different data types. They are treated as byte arrays, by the CPU, but the GPU consumes the
	  data as specified in the shaders.
	- Descriptor sets are bound once per frame at the start of the command buffer.
	- Element buffers also contain vertex data that may be of differing sizes, and again are viewed as byte arrays on the CPU and consumed
	  depending on the graphics pipelines input rate.
	- Element buffers have two bindings, once for static then again for all dynamic.
	- All of these have sub buffers for each renderer, denoted by an offset and size.

Where do push constants come in?
	- Things like camera position

Rendering with a different camera, e.g. for Rainbow Six Siege's cams, or for split screen?
	- Need another scene traversal with newly build RendererCommands and another set of descriptors for each camera active in the scene.
	- Perhaps do the above on a separate thread for each camera and then fill secondary command buffers and combine at end of frame.
	- Need one command pool per camera (i.e. per thread) and per frame queue length (num swap chain images)
	- Note, must do the scene graph update traversal (for game scripts) before this.

How does the scene graph work?
	- The scene graph is a tree structure of nodes each holding a single entity and pointers to child nodes.
	- The graph also has a subgraph called the RenderGraph which only contains entities which have graphics components. In this there
	  are some more specialist node types that can allow for optimisation:
		- NodeGroup nodes define that if that node is not rendered, then all child nodes are also not rendered. Culling tests are
		  only performed for this node, and it is assumed to cover all children.
	- NodeInstances
		- Rather than having full overhead of an entity, components, and loading with meshes and materials, this node specifies an
		  instance count and the per instance parameters required for them (e.g. transforms). This saves a bit of memory and some checks.

Notes:
 - May need to require immediate mode on GPU
 - https://www.saschawillems.de/blog/2018/06/08/multiview-rendering-in-vulkan-using-vk_khr_multiview/ for split screen?